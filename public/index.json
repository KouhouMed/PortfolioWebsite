[{
    "title": "Supervised Learning algorithms with R (part 2)",
    "date": "",
    "description": "2nd part of the previous article about some of the most common supervised learning algorithms and their implementation with R",
    "body": "In a previous article, we have explained the principles of K-Nearest Neighbours and Decision Trees and seen hands-on examples of how these models can be created using R programming language. In this article, we will continue on the same track and see other supervised learning algorithms, namely Linear Regression and Logistic Regression.\nRegression is one of the most important fields in statistics and machine learning. Its objective is to find relationships between variables. On use case of regression is when you try to find out how some features of employees \u0026ndash; such as experience, role, education and city \u0026ndash; influence their salaries. To solve this problem, we apply regression on a dataset comprised of a large enough amount observations, each representing the data (features) related to each employee.\nWhen do we need Regression?\nBasically, regression is used to answer the following questions :\n Does X influence Y? If the answer is yes, how is Y influenced? In other words, how Y is related to each of the input features, and how these features are correlated?  Regression is almost always present in decision-making processes in all fields such as economy, demographic analysis, real estate industry, education, etc.\nLinear Regression  Linear Regression is the simplest regression method and the most widely used because of the ease of interpreting results. It is a predictive model used to predict the outcome value of an outcome variable (dependent variable) Y based on one or more input predictor variables (independent variables) X. The aim is to establish a linear relationship (a mathematical formula) between the predictor variable(s) and the response variable, so that we can use this formula to estimate the value of the response Y, when only the predictors values are known.\n A linear regression between dependent variable $Y$ and a set of independent variables $X=(x_1,\u0026hellip;,x_n)$ assumes that there is a linear relationship between $X$ and $Y$, i.e. : $$Y=\\beta_0 + \\beta_1x_1 + \u0026hellip; + \\beta_nx_n + \\varepsilon$$ The equation above is called the regression equation. $\\beta_0, \\beta_1, \u0026hellip;, \\beta_n$ are called the regression coefficients, and $\\varepsilon$ is the random error.\nLinear regression calculates the predicted weights $\\hat{\\beta}_0, \\hat{\\beta}_1, \u0026hellip;, \\hat{\\beta}_n$ that define the regression function : $$f(X)=\\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \u0026hellip; + \\hat{\\beta}_nx_n$$\nThe estimated response $f(X^{(i)})$ of each observation $i=1,\u0026hellip;,N$ should be as close as possible to the corresponding actual response $Y^{(i)}$. The differences $Y^{(i)}-f(X^{(i)})$ for all the observations $i=1,\u0026hellip;,N$ are called residuals. Regression is about determining the best predicted weights, that is the weights corresponding to the smallest residuals.\nIn order to obtain the best weights, we try to minimize the sum of squared residuals (SSR) for all observations $i=1,\u0026hellip;,N$ : $$SSR=\\sum_i (Y^{(i)}-f(X^{(i)}))^2$$ This method is called the method of ordinary least squares.\n\r\rResiduals are the distance between the observed value and the fitted value.\r\r\rAfter fitting our model, we would eventually want to measure how good the fit is. One way to do it is by calculating $R^2$ also called Coefficient of determination.\nThe variation of the actual responses $Y^{(i)}$ is partly due to their dependence on $X$. But there is also a part of the overall variation that is intrinsic to the output itself.\n$$R^2=\\dfrac{\\text{Variance explained by the model}}{\\text{Total variance}}=1-\\dfrac{SSR}{TSS}$$\n$SSR$ : sum of squared residuals\n$TSS$ : total sum of squares\n$R^2$ is the amount of variation in $Y$ that can be explained by its relationship with $X$. Usually, the larger the $R^2$, the better the regression model fits your observations and can better explain the variation of the output with different inputs. $R^2=1$ corresponds to the case where $SSR=0$ , which is the perfect fit.\n\r\r$R^2=15\\%$\r\r\r\r$R^2=85\\%$\r\r\r\r\r.inline-block {\rdisplay: inline-block;\r}\r\rSimple linear regression Simple Linear Regression is the simplest case of linear regression as it only involves a single independent variable $X=x$.\n\r\rSimple Linear Regression plot\r\r\rSimple linear regression helps us summarize and study relationships between two continuous (quantitative) variables $X=x$ and $Y$.\n",
    "ref": "/blog/supervised-algorithms-with-r-2/"
  },{
    "title": "Supervised Learning algorithms with R (part 1)",
    "date": "",
    "description": "We will discover some of the most common supervised learning algorithms and their implementation with R programming language",
    "body": "As I mentioned in an earlier post, machine learning algorithms are categorized into three main types :\n Supervised learning algorithms Unsupervised learning algorithms Reinfocement learning algorithms  In this article, we will only talk about some of machine learning supervised algorithms. We\u0026rsquo;ll also see examples using R programming language.\nFirst of all, I would like to remind you that supervised learning is a machine learning algorithm that tries to find a function mapping an input to a given output based on a set of examples. Basically, each of these examples (called training set) consists of the input $X$ and the desired output $Y$. In other words, we try to approximate the function $f$ such that $f(X)=Y$ using previously labelled data as learning examples. The performance of such a model is evaluated upon its ability to generalize onto new data that is unlabeled.\nK-Nearest Neighbours K-Nearest Neighbours (KNN) is a simple Machine Learning algorithm based on Supervised Learning technique. It can be used for both regression and classification, but it is mostly used in classification.\nSuppose we have two categories : Category $A$ and Category $B$, and we have a new data point $x$ that we want to know to which category it belongs.\n\rKNN algorithm is comprised of the following steps :\n Step 1 : Select K, number of neighbours Step 2 : Calculate the distance of between $x$ and each of the other data points (we can use the Euclidian distance or others) Step 3 : Take the K nearest neighbors as per the calculated Euclidean distance. Step 4 : Among these k neighbors, count the number of the data points in each category. Step 5 : Assign the new data points to that category for which the number of the neighbor is maximum.  NB 1 : The Euclidian distance between two points $a_1(x_1,y_1)$ and $a_2(x_2,y_2)$ is calculated as follows : $$d(a_1,a_2)=\\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}$$\n\rA more general formula in an n-dimensional space is : $$d(x,y)=\\sqrt{\\sum_{i=1}^{n}(x_i-y_i)^2}$$ where $x(x_1,...,x_n)$ and $y(y_1,...,y_n)$.\rNB 2 : As can be seen, there are no parameters that need to be learned during training to determine whether a new observation belongs to class ùê¥ or ùêµ. The only parameter used in K-Nearest Neighbours is K, which is a predetermined value. The algorithm simply works by looking at the training samples, calculating distances and finding the K examples in the training set that are closest to the new observation. Thus, KNN is a non-parametric, supervised (needs training labels) learning algorithm.\nNB 3 : KNN does support categorical variables as features, simply because we cannot calculated the distance from them.\nThe hands-on example1 that we will work on will use the Sonar data set (signals) from mlbench library. Sonar is a system for the detection of objects under water and for measuring the water\u0026rsquo;s depth by emitting and detecting sound pulses (the complete description can be found ‚Üíhere). For our purposes, this is a two-class (class $R$ and class $M$) classification task with numeric data.\nFirst of all, let\u0026rsquo;s install the required libraries :\n# install the packages (note: this may take some time) install.packages(\u0026#34;class\u0026#34;) install.packages(\u0026#34;caret\u0026#34;) install.packages(\u0026#34;mlbench\u0026#34;) install.packages(\u0026#34;e1071\u0026#34;) library(class) library(caret) require(mlbench) library(e1071) library(base) require(base) Step 1 : Loading the data Let\u0026rsquo;s load the Sonar dataset and look at the first five rows :\ndata(Sonar) head(Sonar) Step 2 : Preparing and exploring the data nrow(Sonar) ncol(Sonar) This will display the number of lines (208 observations) and the number of columns (61 variables), all numerical except for the Class variable which is categorical.\nLet\u0026rsquo;s check how many $R$ classes and $M$ classes Sonar contains :\nbase::table(Sonar$Class) Now let\u0026rsquo;s see if it contains any NA values in its columns :\napply(Sonar, 2, function(x) sum(is.na(x))) We are going to manually split Sonar into training and test sets. Here, we will dedicate 70% of the dataset for traing, and the rest for testing :\nSEED \u0026lt;- 123 set.seed(SEED) data \u0026lt;- Sonar[base::sample(nrow(Sonar)), ] # shuffle data first bound \u0026lt;- floor(0.7 * nrow(data)) df_train \u0026lt;- data[1:bound, ] df_test \u0026lt;- data[(bound + 1):nrow(data), ] cat(\u0026#34;Number of training and test samples are \u0026#34;, nrow(df_train), nrow(df_test)) Now, let\u0026rsquo;s create the following dataframes :\nX_train \u0026lt;- subset(df_train, select=-Class) y_train \u0026lt;- df_train$Class X_test \u0026lt;- subset(df_test, select=-Class) # exclude Class for prediction y_test \u0026lt;- df_test$Class tep 3 : Training a model on data Now, we are going to use knn function from class library with $K=3$ :\nknn_model \u0026lt;- knn(train=X_train, test=X_test, cl=y_train, # class labels k=3) knn_model If you run the code above, you\u0026rsquo;ll see the prediction made by knn_model with $K=3$ on X_test.\nStep 4 : Evaluate the model performance In order to see how many classes have been correctly or incorrectly classified, we can create a confusion matrix as follows :\nconf_mat \u0026lt;- base::table(y_test, knn_model) conf_mat To compute the accuracy, we sum up all the correctly classified observations (located in diagonal) and divide it by the total number of classes :\ncat(\u0026#34;Accuracy: \u0026#34;, sum(diag(conf_mat))/sum(conf_mat)) To assess whether $K=3$ is a good choice and see whether $K=3$ leads to overfitting/underfitting the data, we could use knn.cv which does the leave-one-out cross-validations for training set (i.e., it singles out a training sample one at a time and tries to view it as a new example and see what class label it assigns).\nknn_loocv \u0026lt;- knn.cv(train=X_train, cl=y_train, k=3) knn_loocv Let\u0026rsquo;s create a confusion matrix to compute the accuracy of the training labels y_train and the cross-validated predictions knn_loocv, same as the above :\nconf_mat_cv \u0026lt;- base::table(y_train, knn_loocv) conf_mat_cv cat(\u0026#34;LOOCV accuracy: \u0026#34;, sum(diag(conf_mat_cv)) / sum(conf_mat_cv)) The difference between the cross-validated accuracy and the test accuracy shows that $K=3$ leads to overfitting. Perhaps we should change $K$ to lessen the overfitting.\nStep 5 : Improve the performance of the model There are a couple things we can do in order to improve the performance of our model :\n Centering and scaling data : these are forms of preprocessing numerical data (not suitable for categorical data). Centering a variable means subtracting the mean of the variable from each data point so that the new variable\u0026rsquo;s mean is 0. And scaling consists of multiplying each data point by a constant in order to alter the range of the data. Performing a cross-vaidation : this consists of dividing the data into a finite number of subsets. Through each iteration, a subset is set aside, and the remaining subsets are used as the training set. The subset that was set aside is used as the test set (prediction). We will use caret library for this purpose.  This is a method of cross-referencing the model built using its own data :\nSEED \u0026lt;- 2016 set.seed(SEED) # create the training data 70% of the overall Sonar data. in_train \u0026lt;- createDataPartition(Sonar$Class, p=0.7, list=FALSE) # create training indices ndf_train \u0026lt;- Sonar[in_train, ] ndf_test \u0026lt;- Sonar[-in_train, ] Here, we specify the cross-validation method we want to use to find the best $K$ in grid search.\n# lets create a function setup to do 5-fold cross-validation with 2 repeat. ctrl \u0026lt;- trainControl(method=\u0026#34;repeatedcv\u0026#34;, number=5, repeats=2) nn_grid \u0026lt;- expand.grid(k=c(1,3,5,7)) nn_grid set.seed(SEED) best_knn \u0026lt;- train(Class~., data=ndf_train, method=\u0026#34;knn\u0026#34;, trControl=ctrl, preProcess = c(\u0026#34;center\u0026#34;, \u0026#34;scale\u0026#34;), # standardize tuneGrid=nn_grid) best_knn Running the code above, you\u0026rsquo;ll find out that $K=1$ has the highest accuracy from repeated cross-validation.\nDecision Trees Decision Trees are one of the most powerful predictive classification models. They are based on the analysis of a set of data points that describe the type of object we want to classify. A Decision tree is a flowchart like tree structure, where each internal node denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (terminal node) holds a class label.\n\rIn our practical example2, we\u0026rsquo;ll try to classify a set of mushrooms as either edible or poisonous based on features like its cap type, color, odor, shape of its stalk, etc.\n\r\rExample of mushroom features and their classification\r\r\rThe algorithm behind Decision trees uses probabilities. For example, if many mushrooms that have large caps are poisonous, the algorithm will assume that the probability of large-cap mushrooms being poisonous is high. When the model is complete, we have a tree-like structure composed of what are called decision nodes, which ask our data point questions about its features, and leaf nodes, which tells us what classification the decision tree thinks our data point is.\n\r\rExample of a possible Decision Tree describing mushrooms\r\r\rThe goal of a decision tree is to split the dataset on based on attributes. But how to find the best feature in each node to split?\nTo answer this question, let\u0026rsquo;s first define the Entropy.\nEntropy is the amount of information disorder, or the amount of randomness in the data. It is calculated for each node and it depends on how much random data that node contains. In decision tree we are looking for a trees that have smallest entropy in their nodes. The entropy is used to calculate the homogeneity of the samples in that node. If the samples are completely homogeneous the entropy is zero and if the sample is an equally divided it has entropy of one. It means, if all data in a node are either poisonous or edible, then the entropy is zero, but if the half of data are poisonous and other half are edible, then the entropuy is one. In our example, we can calculate the Entropy of our target class using the following formula :\n$$Entropy = - p(edible)log(p(edible)) - p(poisonous)log(p(poisonous))$$\nDecision trees use another metric on which decisions are based : Information gain. We can think of it as the opposite of entropy. The more randomness decreases, the more information we gain, and vice-versa. Thus, while building a decision tree, we choose the attributes with the highest information gain. $$\\text{Information Gain = entropy(parent) ‚Äì [average entropy(children)]}$$\nAlgorithm :\n 1. Calculate entropy of the target field (the class label) for whole dataset.\r2. For each attribute:\r- split the dataset on the attribute\r- calculate entropy of the target field on splited dataset, using the attribute values\r- calculate the information gain of the attribute\r3. select the attribute that has the largest informmation gain\r4. Branch the tree using the selected attribute\r5. stop, if it is a node with entropy of 0, otherwise jump to step2\r Decision tree with R We will start by loading the data. We\u0026rsquo;ll use UCI\u0026rsquo;s Mushroom dataset. Since this dataset is not inbuilt into R, we need to download it and load it into R :\ndownload.file(\u0026#34;https://ibm.box.com/shared/static/dpdh09s70abyiwxguehqvcq3dn0m7wve.data\u0026#34;, \u0026#34;mushroom.data\u0026#34;) After downloading the file, we need to create a data frame to house the observations in the dataset. Since the dataset is structured using comma-separated values, we can use the read.csv function :\nmushrooms \u0026lt;- read.csv(\u0026#34;mushroom.data\u0026#34;, header = F) mushrooms Once that\u0026rsquo;s done, we have the data loaded up. However, the way that it is structured isn\u0026rsquo;t the most intuitive. In the code cell below, we are adding the column names to the data frame with the colnames function. Additionally, since our data frame is composed of factors, we can rename some of these factors to something more easily understood by us using levels.\n# Define column names for the mushrooms data frame. colnames(mushrooms) \u0026lt;- c(\u0026#34;Class\u0026#34;,\u0026#34;cap.shape\u0026#34;,\u0026#34;cap.surface\u0026#34;,\u0026#34;cap.color\u0026#34;,\u0026#34;bruises\u0026#34;,\u0026#34;odor\u0026#34;,\u0026#34;gill.attachment\u0026#34;,\u0026#34;gill.spacing\u0026#34;, \u0026#34;gill.size\u0026#34;,\u0026#34;gill.color\u0026#34;,\u0026#34;stalk.shape\u0026#34;,\u0026#34;stalk.root\u0026#34;,\u0026#34;stalk.surface.above.ring\u0026#34;, \u0026#34;stalk.surface.below.ring\u0026#34;,\u0026#34;stalk.color.above.ring\u0026#34;,\u0026#34;stalk.color.below.ring\u0026#34;,\u0026#34;veil.type\u0026#34;,\u0026#34;veil.color\u0026#34;, \u0026#34;ring.number\u0026#34;,\u0026#34;ring.type\u0026#34;,\u0026#34;print\u0026#34;,\u0026#34;population\u0026#34;,\u0026#34;habitat\u0026#34;) head(mushrooms) # Define the factor names for \u0026#34;Class\u0026#34; levels(mushrooms$Class) \u0026lt;- c(\u0026#34;Edible\u0026#34;,\u0026#34;Poisonous\u0026#34;) # Define the factor names for \u0026#34;odor\u0026#34; levels(mushrooms$odor) \u0026lt;- c(\u0026#34;Almonds\u0026#34;,\u0026#34;Anise\u0026#34;,\u0026#34;Creosote\u0026#34;,\u0026#34;Fishy\u0026#34;,\u0026#34;Foul\u0026#34;,\u0026#34;Musty\u0026#34;,\u0026#34;None\u0026#34;,\u0026#34;Pungent\u0026#34;,\u0026#34;Spicy\u0026#34;) # Define the factor names for \u0026#34;print\u0026#34; levels(mushrooms$print) \u0026lt;- c(\u0026#34;Black\u0026#34;,\u0026#34;Brown\u0026#34;,\u0026#34;Buff\u0026#34;,\u0026#34;Chocolate\u0026#34;,\u0026#34;Green\u0026#34;,\u0026#34;Orange\u0026#34;,\u0026#34;Purple\u0026#34;,\u0026#34;White\u0026#34;,\u0026#34;Yellow\u0026#34;) head(mushrooms) Now let\u0026rsquo;s build our model. We are going to use rpart library to create the decision tree, and rpart.plot to visualize it. But first, install rpart.plot if it\u0026rsquo;s not already installed.\ninstall.packages(\u0026#34;rpart\u0026#34;) install.packages(\u0026#34;rpart.plot\u0026#34;) # Import our required libraries library(rpart) library(rpart.plot) To create our decision tree model, we can use the rpart function. rpart is simple to use: you provide it a formula, show it the dataset it is supposed to use and choose a method (either \u0026ldquo;class\u0026rdquo; for classification or \u0026ldquo;anova\u0026rdquo; for regression).\nA great trick to know when handling very large structured datasets (our dataset has over 20 columns we want to use!) is that in formula declarations, one can use the . operator as a quick way of designating \u0026ldquo;all other columns\u0026rdquo; to R. You can also print the Decision Tree model to retrieve a summary describing it.\n# Create a classification decision tree using \u0026#34;Class\u0026#34; as the variable we want to predict and everything else as its predictors. myDecisionTree \u0026lt;- rpart(Class ~ ., data = mushrooms, method = \u0026#34;class\u0026#34;) # Print out a summary of our created model. print(myDecisionTree) Now that we have our model, we can draw it to gain a better understanding of how it is classifying the data points. We can use the rpart.plotfunction \u0026ndash; a specialized function for plotting trees \u0026ndash; to render our model. This function takes on some parameters for visualizing the tree in different ways \u0026ndash; try changing the type (from 1 to 4) parameter to see what happens!\nIf you run the code above, you\u0026rsquo;ll see that our decision tree has perfect accuracy when classifying poisonous mushrooms, and almost perfect accuracy when dealing with edible ones.\nnewCase \u0026lt;- mushrooms[10,-1] newCase predict(myDecisionTree, newCase, type = \u0026#34;class\u0026#34;) Model accuracy :\nLet\u0026rsquo;s split our dataset into traing set and test set :\n## 75% of the sample size n \u0026lt;- nrow(mushrooms) smp_size \u0026lt;- floor(0.75 * n) ## set the seed to make your partition reproductible set.seed(123) train_ind \u0026lt;- base::sample(c(1:n), size = smp_size) mushrooms_train \u0026lt;- mushrooms[train_ind, ] mushrooms_test \u0026lt;- mushrooms[-train_ind, ] newDT \u0026lt;- rpart(Class ~ ., data = mushrooms_train, method = \u0026#34;class\u0026#34;) result \u0026lt;- predict(newDT, mushrooms_test[,-1], type = \u0026#34;class\u0026#34;) head(result) head(mushrooms_test$Class) base::table(mushrooms_test$Class, result)   Credit: Ehsan M. Kermani \u0026#x21a9;\u0026#xfe0e;\n Credit: Saeed Aghabozorgi, Walter Gomes de Amorim Junior \u0026#x21a9;\u0026#xfe0e;\n   ",
    "ref": "/blog/supervised-learning-algorithms-r/"
  },{
    "title": "What is machine learning ?",
    "date": "",
    "description": "Why is this field getting so much attention ? ü§î",
    "body": " \u0026ldquo;What we want is a machine that can learn from experience\u0026rdquo;\nAlan Turing, 1947\n What is machine learning ? Machine learning is a form of artificial intelligence (AI) that allows a system to learn from data without explicitly programming the instructions to be executed. However, machine learning is not a simple process. As the algorithms ingest the training data, it becomes possible to create more accurate models based on that data. A machine learning model is the output generated when you train your machine learning algorithm with data. After training, when you provide input data to a model, you receive an output result. For example, a predictive algorithm creates a predictive model. Then, when you provide data to the predictive model, you receive a forecast that is determined by the data that was used to train the model.\nIterative learning from data Machine learning allows models to train on datasets before being deployed. Some machine learning models are online and operate continuously. This iterative process of inline models improves the types of associations established between data elements. Due to their complexity and size, these trends and associations may not be detected by a human observer. Once a model has been trained, it can be used in real time to learn from the data. Improvements in accuracy result from the training and automation process that is part of machine learning.\nMachine learning approaches Machine learning techniques are needed to improve the accuracy of predictive models. Depending on the nature of the business problem being addressed, there are different approaches that vary depending on the type and volume of data. In this section, we discuss the categories of machine learning.\nSupervised learning Supervised learning usually begins with a well-defined data set and some understanding of how that data is classified. The goal of supervised learning is to uncover patterns in data and apply them to an analytical process. These data include characteristics associated with labels that define their meaning. You can, for example, create a machine learning application that can distinguish between millions of animals, based on pictures and written descriptions.\nUnsupervised learning Unsupervised learning is used when the problem requires a massive amount of unlabeled data. For example, social media apps like Twitter, Instagram, and Snapchat all mine very large amounts of untagged data. To understand the meaning of this data, it is necessary to use algorithms that classify the data according to the trends or clusters they detect. Unsupervised learning leads an iterative process, analyzing data without human intervention. It is used with spam detection technology sent by e-mail. Normal emails and spam have too many variables for an analyst to tag spam emails sent in bulk. In contrast, machine learning discriminants, based on clustering and association, are applied to identify unwanted emails.\nReinforcement learning Reinforcement learning is a model of behavioral learning. The algorithm receives feedback from the data analysis and guides the user to the best result. Reinforcement learning differs from other types of supervised learning in that the system is not trained with a sample data set. Instead, the system learns instead through a trial and error method. Therefore, a sequence of successful decisions results in the strengthening of the process, because it is the process that most effectively solves the problem at hand.\nNeural networks and deep learning  Deep learning is a specific method of machine learning that integrates neural networks in successive layers in order to learn data iteratively. Deep learning is especially useful when trying to spot trends from unstructured data. Complex deep learning neural networks are designed to emulate how the human brain works, so computers can be trained to deal with ill-defined abstractions and problems. Most five-year-olds easily distinguish the face of their teacher from that of the officer responsible for taking them across the crosswalk. On the other hand, the computer must do a considerable amount of work to identify each face. Neural networks and deep learning are often used in image recognition, oral communication and digital vision applications.\rMachine learning vs Statistical Modeling Statistical modeling and machine learning can be mixed up sometimes. But there is a difference between these two concepts. Machine Learning is an algorithm that can learn from data without being explicitly programmed or relying on standard programming practices. Here are some important facts about machine learning :\n Machine learning is a newer field of study than statistics (machine learning was invented in 1959, whereas statistics originated in the 17th century) Machine learning can result in more detailed information than statisticl modeling Machine learning is a subfield of computer science and A.I and contributes to building systems that can learn from data without explicit programming Finally, machine learning uses fewer assumptions than statistcal modeling  Statistical Modeling is the formalization of relationships between variables in the form of mathematical equations. It is a subfield of math that deals with finding relationships between variables to predict outcomes. It deals with a small amount of data with fewer attributes and, as such, there is a good chance that over-fitting will occur.\nStatistical modeling requires the modeller to understand the relation and implementation that a variable has on an equation, in an effort to best \u0026ldquo;estimate\u0026rdquo; the function output to a certain error.\nOn the other hand, machine Learning requires minimal human effort, as the workload involved in computing is placed squarely on the machine. Furthermore, Machine Learning has a strong predictive power, as the machine is \u0026ldquo;fit\u0026rdquo; and \u0026ldquo;trained\u0026rdquo; to find patterns in the data.\n    Machine Learning Statistical Modeling      Network, Graphs Model    Weights Parameters    Learning Fitting    Supervised Learning Regression/Classification    Unsupervised Learning Density Estimation/Clustering    ",
    "ref": "/blog/ml/"
  },{
    "title": "About Me",
    "date": "",
    "description": "Welcome to my website!üòÉ",
    "body": "‚Üí[CV] ‚Üê This website will give you an overview of my academic and professional career, as well as some of my interests. And you will also find some articles about different topics.\nI am a Data Engineering student in my 2nd year at the National Institute of Posts and Telecommunications, Rabat, Morocco.\nMy educational career was marked by a lot of moving between cities and compuses (Akka, Tata, Taroudant, Agadir, Rabat\u0026hellip;). And this made a significant contribution in the process of developping my personality, building my network, and acquiring a variety of skills.\nI had my baccalaureate in Mathematical Sciences in 2017 with an Honours degree at the Technical Highschool, Taroudant, Morocco. I then went to Mohammed Reda Slaoui Preparatory Classes in Agadir the same year and studied Maths And Physics for two years. In 2019, I went to the National Institute of Posts and Telecommunications in Rabat where I\u0026rsquo;m still studying Data Engineering in my 2nd year now.\nMain interests and projects My main interests are Data Science, Machine Learning, and Marketing Analysis. The first time I worked on a machine learning project was in my final year at the Preparatory Classes as part of the TIPE (Traveaux d\u0026rsquo;Initiative Personnelle Encadr√©s), in which I trained a convolutional neural network to guide a vehicle through an empty track. I used Udacity\u0026rsquo;s Self-Driving Car Simulator for generating the training and the testing data (TRAINING MODE), as well as for testing the performance of the car after I paired it with the trained model (AUTONOMOUS MODE).\nI had so much fun working on this project. I especially enjoyed the process of collecting data and testing the car on the simulator.\nI also carried out a web scrapping project in which I extracted around 13 000 items on the famous advertisements website Avito.ma using requests and BeautifulSoup and performed statistical studies on them.\nI love NLP (Natural Language Processing), and my latest mini-project was about determining, among a set of tweets, which relate to a disaster and which don\u0026rsquo;t, using NLP along with different machine learning algorithms, and then I compared the efficiency of each one of these algorithms.\nBadges and certificates I am glad that I had the chance to take many online courses in different domains especially those related to my area of expertise and earn a certificate for each one of them :\n   Badge/Certificate Date Issued by     Using Databases With Python November 2020 Coursera   R 101 November 2020 IBM   HCIA Big Data August 2020 Huawei   Natural Language Processing July 2020 Coursera (With Honors)   Scrum Foundation Professional Certificate (SFPC) August 2020 CertiProf   Lifelong Learning August 2020 CertiProf   HCIA Artificial Intelligence July 2020 Huawei   Data Science for Business - Level 1 May 2020 IBM   Data Science Orientation April 2020 IBM   Python For Data Science April 2020 IBM   Data Science Hands-On With Open Source Tools April 2020 IBM   Advanced Google Analytics April 2020 Google   Google Analytics For Beginners Mars 2020 Google    \r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\rExtracurricular \u0026amp; Volunteering Experiences As an active member of CIT Club, I\u0026rsquo;ve proudly been part of the organization staff of its two main events (JNJD and IDEH) last year. And this year, I have been elected Treasurer of the club.\nI\u0026rsquo;ve also volunteered as an organizer of the 1st edition of the Digital Job Fair.\nHobbies and pastime activities I\u0026rsquo;m so passionate about photography üì∏ , music üé∂, and reading üìö\nThanks for passing by! üòÑ\nPlease feel free to contact me.\n\r* {\rbox-sizing: border-box;\r}\rh1 {\rtext-align: center;\r}\r.outer-grid {\rdisplay: flex;\rflex-wrap: wrap;\rpadding: 0 4px;\r}\r.inner-grid {\rflex: 25%;\rmax-width: 25%;\rpadding: 0 4px;\r}\r.inner-grid img {\rmargin-top: 8px;\rwidth: 100%;\rpadding: 10px;\r}\r@media screen and (max-width: 800px) {\r.inner-grid {\rflex: 50%;\rmax-width: 50%;\r}\r}\r@media screen and (max-width: 600px) {\r.inner-grid {\rflex: 50%;\rmax-width: 100%;\r}\r}\r  ",
    "ref": "/about/"
  },{
    "title": "What is Blockchain Technology?",
    "date": "",
    "description": "",
    "body": "MathJax Test $$\\int x^3 dx$$\nCode highlighting Test if(FALSE) { \u0026#34;This is a demo for multi-line comments and it should be put inside either a single OR double quote\u0026#34; } myString \u0026lt;- \u0026#34;Hello, World!\u0026#34; print ( myString) ",
    "ref": "/blog/blockchain/"
  },{
    "title": "Natural Language Processing (NLP)",
    "date": "",
    "description": "",
    "body": "Lorem est tota propiore conpellat pectoribus de pectora summo.\nRedit teque digerit hominumque toris verebor lumina non cervice subde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc caluere tempus inhospita parcite confusaque translucet patri vestro qui optatis lumine cognoscere flos nubis! Fronde ipsamque patulos Dryopen deorum.\n Exierant elisi ambit vivere dedere Duce pollice Eris modo Spargitque ferrea quos palude  Rursus nulli murmur; hastile inridet ut ab gravi sententia! Nomine potitus silentia flumen, sustinet placuit petis in dilapsa erat sunt. Atria tractus malis.\n Comas hunc haec pietate fetum procerum dixit Post torum vates letum Tiresia Flumen querellas Arcanaque montibus omnes Quidem et  Vagus elidunt \nThe Van de Graaf Canon\nMane refeci capiebant unda mulcebat Victa caducifer, malo vulnere contra dicere aurato, ludit regale, voca! Retorsit colit est profanae esse virescere furit nec; iaculi matertera et visa est, viribus. Divesque creatis, tecta novat collumque vulnus est, parvas. Faces illo pepulere tempus adest. Tendit flamma, ab opes virum sustinet, sidus sequendo urbis.\nIubar proles corpore raptos vero auctor imperium; sed et huic: manus caeli Lelegas tu lux. Verbis obstitit intus oblectamina fixis linguisque ausus sperare Echionides cornuaque tenent clausit possit. Omnia putatur. Praeteritae refert ausus; ferebant e primus lora nutat, vici quae mea ipse. Et iter nil spectatae vulnus haerentia iuste et exercebat, sui et.\nEurytus Hector, materna ipsumque ut Politen, nec, nate, ignari, vernum cohaesit sequitur. Vel mitis temploque vocatus, inque alis, oculos nomen non silvis corpore coniunx ne displicet illa. Crescunt non unus, vidit visa quantum inmiti flumina mortis facto sic: undique a alios vincula sunt iactata abdita! Suspenderat ego fuit tendit: luna, ante urbem Propoetides parte.\n",
    "ref": "/blog/nlp/"
  },{
    "title": "What is Data Science ?",
    "date": "",
    "description": "Why Data Science? What are the prerequisites and skills for Data Science ? What is the lifecyle of a Data Science project? In this article, you will find the answers to these questions and more.",
    "body": " ‚ÄúYou can have data without information, but you cannot have information without data.‚Äù ‚Äì Daniel Keys Moran\n Data science is one of the most debated topics in the industries these days. Its popularity has grown over the years, and companies have started implementing data science techniques to grow their business and increase customer satisfaction. According to a study conducted in 2013, 90% of the world\u0026rsquo;s data has been produced within the previous two years. In 2020, the global amount of data is projected to reach 59 zettabytes. Given the massive amount of data that is being created every second, people in almost all industries have become concerned as to how to make use of all this data being produced. This is when Data Science comes into play.\nWhat is Data Science ? Data science is the domain of study that deals with vast volumes of data using modern tools and techniques to find unseen patterns, derive meaningful information, and make business decisions. Data science uses complex machine learning algorithms to build predictive models.\n \u0026ldquo;Data Scientist (n.): Person who is better at statistics than any software engineer and better at software engineering than any statistician.\u0026rdquo; ‚Äì Josh Wells\n \rData science combines the scientific method, math and statistics, specialized programming, advanced analytics, AI, and even storytelling to uncover and explain the business insights buried in data.It envolves preparing raw data to make it ready for processing, performing data analysis, and presenting the results to reveal patterns abd enable stakeholders to extract useful insights.\nData preparation envolves cleaning, aggregating, and manipulating data in a way that makes it suitable for the desired type of processing. As for analysis, it is requires algorithms and AI models to explore the data and extract patterns. Decisions will be taken based on the predictions that have been with the models.\nIn order for these models to be able to predict reliably, they have to be optimized through training. And the accuracy of the predictions must be validated through scientifically designed tests and experiments.\nWhat are the prerequisites and skills for Data Science ? a data scientist must be able to do the following tasks :\n Apply mathematics, statistics, and the scientific method Use a variety of tools and techniques for evaluating and preparing data (SQL, data mining, data integration methods\u0026hellip;) Extract insights from data using predictive analytics and artificial intelligence (AI), including machine learning and deep learning models Write applications that automate data processing and calculations Tell stories that clearly depict the results of the analysis process (this is usually done through visualization and other techniques) Explain how these results can be used to solve business problems  Technical prerequisites for Data Science Here are some technical demands you should know before setting of a data science journey :\n  Mathematics : data science requires some basic knowledge in certain mathematical concepts such as calculus, linear algebra, probability, statistics, information theory\u0026hellip; data scientists should be familiar with these concepts in order to be able to fully understand the magic behind statistical and machine learning models, and thus be capable of adapting them to their needs.\n  Programming : some programming skills are needed to execute successful data science projects. It is not required to be a hardcore programmer to help analyze widespread parts of data, to write quotes efficiently to explain the problem area and work with big data. Data science works on programming tools like Python and R. These concepts will help the candidate to journey a long way into the expertise of data science.\n  Databases : they are an essential component of data science. Data scientists need to understand how databases work, how to manage them, and how to extract data from them.\n  Machine learning : it is one of the most fundamental concepts of data science. Data Scientists need to have a solid grasp on ML in addition to basic knowledge of statistics.\n     Field Skills Tools     Data Analysis R, Python, Statistics SAS, Jupyter, R Studio, MATLAB, Excel, RapidMiner   Data Warehousing ETL, SQL, Hadoop, Apache Spark Informatica/ Talend, AWS Redshift   Data Visualization R, Python libraries Jupyter, Tableau, Cognos, RAW   Machine Learning Python, Algebra, Machine learning algorithms, Statistics Spark MLib, Mahout, Azure ML studio    \rRange of Technologies Brands in Data Science\r\rData Science Lifecycle \rData Science Lifecycle\r\rData science projects are conducted through several processes, so it is important to have a general structure to follow. There are no rules or stricts procedures to follow in a data science project. However, the following lifecycle outlines the major stages that projects typically execute, often iteratively:\n Business Understanding : This is the first step of all data science projects. It includes two main tasks :   Define objectives: Work with your customer and other stakeholders to understand and identify the business problems. Formulate questions that define the business goals that the data science techniques can target. Identify data sources: Find the relevant data that helps you answer the questions that define the objectives of the project.   Data Collection : It is not an easy process. It involves so many tasks such as identifying your data requirements, deciding on a method of data collection, and finally organizing a data collection plan that synthesizes the most important aspects of your program.\n  Data Preparation : This step is also known as Data Cleaning or Data Wrangling. It includes steps like selecting the relevant data, integrating the data by merging the data sets, cleaning it, handling the missing values by either removing them or imputing them with relevant data, treating erroneous data by removing them, also check for outliers and handle them. Constructing new data, derive new features from existing ones by using the feature engineering. Format the data into the desired structure, remove unwanted columns and features. Data preparation is the most time consuming as it takes up to 80% of the overall project time, yet it‚Äôs the most important step in the entire life cycle.\n  Exploratory Data Analysis (EDA) : This is not a formal process with a strict set of rules. It is rather a mindset that incites you to analyze and investigate your data sets and summarize their main characteristics, often employing data visualization methods. It helps you to figure out the best way to manipulate your data. In this stage, you may want to do the following tasks :\n   Generate questions about your data Search for answers by visualising, transforming, and modelling your data Use what you learn to refine your questions and/or generate new questions   Modeling : Data modeling is the process of creating a visual representation of either a whole information system or parts of it to communicate connections between data points and structures. The goal is to illustrate the types of data used and stored within the system, the relationships among these data types, the ways the data can be grouped and organized and its formats and attributes.Data modeling employs standardized schemas and formal techniques. This provides a common, consistent, and predictable way of defining and managing data resources across an organization, or even beyond.\n  Model Evaluation : In the end we need to evaluate the model by measuring the accuracy (How well the model performs i.e. does it describe the data accurately) and relevance (Does it answer the original question that is set out to answer). We also need to make sure there is a correct balance between performance and generalizability, which means the model created should not be biased and should be a generalized model.\n  Model Deployment : In order to start using a model for practical decision-making, it needs to be effectively deployed into production. Depending on the requirements, the deployment phase can be as simple as generating a report or as complex as implementing a repeatable data science process. In many cases, it will be the customer, not the data analyst, who will carry out the deployment steps. For example, a credit card company may want to deploy a trained model or set of models (e.g., neural networks, meta-learner) to quickly identify transactions, which have a high probability of being fraudulent. However, even if the analyst will not carry out the deployment effort it is important for the customer to understand up front what actions will need to be carried out in order to actually make use of the created models.\n  Conclusion In the end, we won\u0026rsquo;t be wrong to say that Data Science is driving the future of almost all industries as it is the fastest growing discipline in computer science and IT with all its active community and the new tools being developped continuously.\n",
    "ref": "/blog/ds/"
  }]
